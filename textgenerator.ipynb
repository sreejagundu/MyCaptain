{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textgenerator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAkXP-K-9MWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5adf39-4055-46f3-8a31-c696f474bdb6"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUWJtxzk-7lY"
      },
      "source": [
        "file = open('proj.txt').read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqddWnFE9TtJ"
      },
      "source": [
        "# Tokenization\n",
        "# Standardization\n",
        "def tokenize_words(input):\n",
        "  input = input.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(input)\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'),tokens)\n",
        "  return \"\".join(filtered)\n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fntYXZi9Tpt"
      },
      "source": [
        "# Char to numbers\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i) for i,c in enumerate(chars))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuHRonUS9TnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09c0ca0-7bd3-4698-e59a-b4413b594b1c"
      },
      "source": [
        "# check if words to chars or chars to num works\n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print('Total no.of characters:', input_len)\n",
        "print('total vocab:', vocab_len)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no.of characters: 15469\n",
            "total vocab: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-8ZR1wSqLSe"
      },
      "source": [
        "# seq length\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIlmxgGQqLO5",
        "outputId": "1e738031-75b8-43d0-8eaa-05ac2a3d321b"
      },
      "source": [
        "# loop through sequence\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  in_seq = processed_inputs[i:i + seq_length]\n",
        "  out_seq = processed_inputs[i + seq_length]\n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "n_patterns = len(x_data)\n",
        "print('total patterns:', n_patterns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total patterns: 15369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KZrkEG8qLMC"
      },
      "source": [
        "# convert input sequence to np array and so on\n",
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV16vJrbqLJd"
      },
      "source": [
        "# one hot encoding\n",
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN5uygZ-t7fb"
      },
      "source": [
        "# creating model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryjV5Db5t7cE"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CsH8pelt7aC"
      },
      "source": [
        "#saving model\n",
        "filepath = 'model_weigths_saved.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks= [checkpoint]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOMU7A8mt7XW",
        "outputId": "d3dcd4e9-2491-4612-a2f5-1027feb84216"
      },
      "source": [
        "#fit model\n",
        "model.fit(X,y, epochs=10, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "61/61 [==============================] - 238s 4s/step - loss: 3.2726\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.16468, saving model to model_weigths_saved.hdf5\n",
            "Epoch 2/10\n",
            "61/61 [==============================] - 234s 4s/step - loss: 3.0979\n",
            "\n",
            "Epoch 00002: loss improved from 3.16468 to 3.08765, saving model to model_weigths_saved.hdf5\n",
            "Epoch 3/10\n",
            "61/61 [==============================] - 236s 4s/step - loss: 3.0849\n",
            "\n",
            "Epoch 00003: loss improved from 3.08765 to 3.08382, saving model to model_weigths_saved.hdf5\n",
            "Epoch 4/10\n",
            "61/61 [==============================] - 233s 4s/step - loss: 3.0762\n",
            "\n",
            "Epoch 00004: loss improved from 3.08382 to 3.07498, saving model to model_weigths_saved.hdf5\n",
            "Epoch 5/10\n",
            "61/61 [==============================] - 234s 4s/step - loss: 3.0761\n",
            "\n",
            "Epoch 00005: loss improved from 3.07498 to 3.07199, saving model to model_weigths_saved.hdf5\n",
            "Epoch 6/10\n",
            "61/61 [==============================] - 233s 4s/step - loss: 3.0605\n",
            "\n",
            "Epoch 00006: loss improved from 3.07199 to 3.06690, saving model to model_weigths_saved.hdf5\n",
            "Epoch 7/10\n",
            "61/61 [==============================] - 232s 4s/step - loss: 3.0707\n",
            "\n",
            "Epoch 00007: loss did not improve from 3.06690\n",
            "Epoch 8/10\n",
            "61/61 [==============================] - 233s 4s/step - loss: 3.0700\n",
            "\n",
            "Epoch 00008: loss improved from 3.06690 to 3.06550, saving model to model_weigths_saved.hdf5\n",
            "Epoch 9/10\n",
            "61/61 [==============================] - 237s 4s/step - loss: 3.0749\n",
            "\n",
            "Epoch 00009: loss improved from 3.06550 to 3.06433, saving model to model_weigths_saved.hdf5\n",
            "Epoch 10/10\n",
            "61/61 [==============================] - 235s 4s/step - loss: 3.0625\n",
            "\n",
            "Epoch 00010: loss improved from 3.06433 to 3.06359, saving model to model_weigths_saved.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9794526c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C88Cn8XmwHDn"
      },
      "source": [
        "# recompile model with saved weigths\n",
        "filename = 'model_weigths_saved.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dixGUxcEwHAX"
      },
      "source": [
        "# output of model into char\n",
        "num_to_char = dict((i,c) for i,c in enumerate(chars))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_YRa7-yy4Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62516812-3883-4a9a-9053-46b57215d05a"
      },
      "source": [
        "# random seed to help generate\n",
        "start = numpy.random.randint(0, len(x_data)-1)\n",
        "pattern = x_data[start]\n",
        "print('Random Seed:')\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\" oodmankindfavoredamericanwritermarktwain1835191045mainz1840gutenbergdenkmalmedalgutenbergprintingpre \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3siY-1Ey4V7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c7901e-bfe6-4cee-b4cc-eaf370dabc0b"
      },
      "source": [
        "# generate text\n",
        "for i in range(10):\n",
        "  x=numpy.reshape(pattern, (1,len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eeeeeeeeee"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ixYeq-y4St"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}